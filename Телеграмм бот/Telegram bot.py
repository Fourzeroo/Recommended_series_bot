#!/usr/bin/env python
# coding: utf-8

"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ Word2Vec –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π —Å–µ—Ä–∏–π –ø–æ —Å–∏–Ω–æ–ø—Å–∏—Å–∞–º.
"""

import re
import nltk
import numpy as np
import pandas as pd
from gensim.models import Word2Vec
from googletrans import Translator
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from scipy import spatial
import telebot
from telebot import types


# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ pandas
pd.set_option("display.max_columns", None)

# –°–∫–∞—á–∏–≤–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã –¥–ª—è nltk
nltk.download('stopwords')
nltk.download('punkt')

# –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∞ —Å –ø–æ–º–æ—â—å—é Google Translate API
TRANSLATOR = Translator(service_urls=['translate.google.com'])


class ModelWord2Vec:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ Word2Vec.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        final_df (pandas.DataFrame): pandas DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.

    –ê—Ç—Ä–∏–±—É—Ç—ã:
        vectors (list): –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—â–∏—Ö —Å–ª–æ–≤–∞ –≤ DataFrame final_df.
        model (gensim.models.Word2Vec): –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Word2Vec.

    –ú–µ—Ç–æ–¥—ã:
        create_model(self): –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å Word2Vec.
        vec_df(self): –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç DataFrame final_df –≤ DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞.
        predict(self, synopsis: str, df: pd.DataFrame): –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–µ—Ä–∏–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–∏–Ω–æ–ø—Å–∏—Å–∞.
    """

    def __init__(self, final_df):
        self.vectors = []
        self.final_df = final_df
        self.model = None

    def create_model(self):
        """
        –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å Word2Vec.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            gensim.models.Word2Vec: –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Word2Vec.
        """
        self.model = Word2Vec(
            self.final_df["synopsis_tokens"],
            window=6,
            vector_size=50,
            alpha=0.03,
            min_alpha=0.0007,
            negative=20,
            sg=0
        )
        self.model.train(
            self.final_df["synopsis_tokens"],
            total_examples=len(self.final_df["synopsis_tokens"]),
            epochs=1
        )
        return self.model

    def vec_df(self):
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç DataFrame final_df –≤ DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            pandas.DataFrame: DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞.
        """
        for synopsis_tokens in self.final_df["synopsis_tokens"]:
            synopsis_vector = np.zeros(self.model.vector_size)
            num_tokens = 0
            for token in synopsis_tokens:
                if token in self.model.wv.key_to_index:
                    vector = self.model.wv.get_vector(token)
                    synopsis_vector += vector
                    num_tokens += 1
            if num_tokens > 0:
                synopsis_vector /= num_tokens
                self.vectors.append(synopsis_vector)
        self.final_df["vectors"] = self.vectors

        return self.final_df

    def predict(self, synopsis: str, df: pd.DataFrame) -> pd.Series:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–µ—Ä–∏–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Å–∏–Ω–æ–ø—Å–∏—Å–∞.

        –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
            synopsis (str): –°–∏–Ω–æ–ø—Å–∏—Å —Å–µ—Ä–∏–∏.
            df (pd.DataFrame): DataFrame, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –≤–µ–∫—Ç–æ—Ä—ã –∏ –Ω–∞–∑–≤–∞–Ω–∏—è —Å–µ—Ä–∏–π.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            pandas.Series: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–µ—Ä–∏–∏ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –≤–µ–∫—Ç–æ—Ä.
        """
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–∏–Ω–æ–ø—Å–∏—Å
        synopsis = preprocessor_text(synopsis)

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤ –≤ —Å–∏–Ω–æ–ø—Å–∏—Å–µ, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ –º–æ–¥–µ–ª–∏
        lst = set([i for i in synopsis.split() if i in self.model.wv.key_to_index])

        # –ü–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä—ã —Å–ª–æ–≤ –≤ —Å–∏–Ω–æ–ø—Å–∏—Å–µ
        context_word_vectors = [self.model.wv[word] for word in lst]

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤ –¥–ª—è —Å–∏–Ω–æ–ø—Å–∏—Å–∞
        predicted_vector = np.mean(context_word_vectors, axis=0)

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–º –≤–µ–∫—Ç–æ—Ä–æ–º –∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –≤ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö
        similarities = df['vectors'].map(lambda x: 1 - spatial.distance.cosine(
            np.array(object=x, dtype='float16'),
            predicted_vector.astype('float16')
                )
            )

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–µ—Ä–∏–∏ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º —Å—Ö–æ–¥—Å—Ç–≤–æ–º
        return df.iloc[similarities.argmax()]



def preprocess(df: pd.DataFrame) -> pd.DataFrame:
    """
    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        df: –§—Ä–µ–π–º –¥–∞–Ω–Ω—ã—Ö.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        df + —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏.
    """
    df.dropna(subset=['release_year', 'rating', 'cast'], inplace=True)
    df['release_year'] = df['release_year'].astype(int)
    df["synopsis_tokens"] = df["synopsis1"].apply(word_tokenize)
#     df['series_title'] = df['series_title'].drop_duplicates().dropna()

    return df




def preprocessor_text(text: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—É—é —Å—Ç—Ä–æ–∫—É.

    –ê—Ä–≥—É–º–µ–Ω—Ç—ã:
        text: –¢–µ–∫—Å—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é —Ç–µ–∫—Å—Ç–æ–≤—É—é —Å—Ç—Ä–æ–∫—É.
    """
    text = re.sub('<[^>]*>', '', text)

    emoticons = re.findall('(?::|;|=)(?:-)?(?:\)|\(|D|P)', text)
    text = re.sub('[\W]+', ' ', text.lower()) + \
        ' '.join(emoticons).replace('-', '')

    tokens = word_tokenize(text)

    # —É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word not in stop_words]

    processed_text = ' '.join(filtered_tokens)

    return processed_text


bot = telebot.TeleBot('6264185691:AAF7DgTJ0CLAOnMqgGKsE2NgW7g9DqwiSFs')

@bot.message_handler(commands=['start'])
def handle_start(message):
    bot.send_message(
        message.chat.id,
        "–ü—Ä–∏–≤–µ—Ç, {0.first_name}! –Ø –±–æ—Ç —Å–∏—Å—Ç–µ–º—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å–µ—Ä–∏–∞–ª–æ–≤. –í–≤–µ–¥–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ, –∏ —è –¥–∞–º –≤–∞–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é.".format(message.from_user)
    )
@bot.message_handler(commands=['menu'])   
def show_main_menu(message):
    global keyboard
    keyboard = types.ReplyKeyboardMarkup(resize_keyboard=True)
    markup = types.ReplyKeyboardMarkup(resize_keyboard=True)
    button1 = types.KeyboardButton("‚úíÔ∏è –ü–æ–≤—Ç–æ—Ä–Ω–æ –≤–≤–µ—Å—Ç–∏ –æ–ø–∏—Å–∞–Ω–∏–µ")
    button2 = types.KeyboardButton("‚ÑπÔ∏è –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–≤–µ–¥–µ–Ω–∏—è –æ —Å–µ—Ä–∏–∞–ª–µ")
    markup.add(button1, button2)
    bot.send_message(message.chat.id, text="–ú–µ–Ω—é:", reply_markup=markup)
    
    
@bot.message_handler(func=lambda message: True)
def handle_message(message):
    global predict
    df = pd.read_csv(
        "C:/Users/fourz/OneDrive/–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª/Word2Vec/filter_data.csv",
        usecols=lambda column: column != 'Unnamed: 0' and column != 'synopsis_tokens'
    )
    synaps = message.text
    
    if synaps == "/start":  
        bot.send_message(
            message.chat.id,
            "–ü—Ä–∏–≤–µ—Ç, {0.first_name}! –Ø –±–æ—Ç —Å–∏—Å—Ç–µ–º—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å–µ—Ä–∏–∞–ª–æ–≤. –í–≤–µ–¥–∏—Ç–µ –æ–ø–∏—Å–∞–Ω–∏–µ, –∏ —è –¥–∞–º –≤–∞–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é.".format(message.from_user)
        )
        return  

    if message.content_type == 'text' and (
        message.text.startswith("‚úíÔ∏è") or
        message.text.startswith("‚ÑπÔ∏è") or
        message.text.endswith('üé≠') or 
        message.text.endswith('‚≠êÔ∏è') or
        message.text.endswith('üìÖ') or
        message.text.endswith('üìù') or
        message.text.endswith('üé¨') or
        message.text.endswith('üåê')
    ):
        return 

    final_df = preprocess(df)

    pp = preprocessor_text(synaps)

    model_word2vec = ModelWord2Vec(final_df)
    model = model_word2vec.create_model()

    emb = model_word2vec.vec_df()

    pred = model_word2vec.predict(pp, final_df)

    predict = pred[['series_title', 'release_year', 'runtime', 'genre', 'rating', 'cast', 'synopsis', 'end_year']]
    
    bot.send_message(message.chat.id, f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {predict['series_title']}")


@bot.message_handler(func=lambda message: True)
def handle_text_message(message):
    
    handle_message(message)
    if message.text.startswith("‚úíÔ∏è –ü–æ–≤—Ç–æ—Ä–Ω–æ –≤–≤–µ—Å—Ç–∏ –æ–ø–∏—Å–∞–Ω–∏–µ"):
        bot.send_message(message.chat.id, '–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç:', reply_markup=keyboard)
        handle_message(message)
#         show_main_menu(message)

    elif message.text.startswith("‚ÑπÔ∏è –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–≤–µ–¥–µ–Ω–∏—è –æ —Å–µ—Ä–∏–∞–ª–µ"):
        bot.send_message(message.chat.id, message.text)
        markup = types.ReplyKeyboardMarkup(resize_keyboard=True, row_width=3)
        btn1 = types.KeyboardButton("–£–∑–Ω–∞—Ç—å –∞–∫—Ç–µ—Ä—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤ üé≠")
        btn2 = types.KeyboardButton("–£–∑–Ω–∞—Ç—å —Ä–µ–π—Ç–∏–Ω–≥ —Å–µ—Ä–∏–∞–ª–∞ ‚≠êÔ∏è")
        btn3 = types.KeyboardButton("–£–∑–Ω–∞—Ç—å –≥–æ–¥—ã –≤—ã—Ö–æ–¥–∞ —Å–µ—Ä–∏–∞–ª–∞ üìÖ")
        btn4 = types.KeyboardButton("–£–∑–Ω–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ üìù")
        btn5 = types.KeyboardButton("–£–∑–Ω–∞—Ç—å –∂–∞–Ω—Ä üé¨")
        back = types.KeyboardButton("–ü–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ —Ñ–∏–ª—å–º üåê")
        markup.add(back)
        markup.row(btn2, btn3, btn4)
        markup.add(btn5, btn1)
        bot.send_message(message.chat.id, text="–í—ã–±–µ—Ä–∏—Ç–µ —á–µ–≥–æ —Ö–æ—Ç–∏—Ç–µ —É–∑–Ω–∞—Ç—å:", reply_markup=markup)

        
    elif message.text.startswith("–£–∑–Ω–∞—Ç—å –∞–∫—Ç–µ—Ä—Å–∫–∏–π —Å–æ—Å—Ç–∞–≤ üé≠"):
        bot.send_message(message.chat.id, f"–ê–∫—Ç–µ—Ä—ã: {predict['cast']}")
        show_main_menu(message)

    elif message.text.startswith("–£–∑–Ω–∞—Ç—å —Ä–µ–π—Ç–∏–Ω–≥ —Å–µ—Ä–∏–∞–ª–∞ ‚≠êÔ∏è"):
        bot.send_message(message.chat.id, f"–†–µ–π—Ç–∏–Ω–≥ —Å–µ—Ä–∏–∞–ª–∞: {predict['rating']}")
        show_main_menu(message)

    elif message.text.startswith("–£–∑–Ω–∞—Ç—å –≥–æ–¥—ã –≤—ã—Ö–æ–¥–∞ —Å–µ—Ä–∏–∞–ª–∞ üìÖ"):
        bot.send_message(message.chat.id, f"–ì–æ–¥—ã –≤—ã—Ö–æ–¥–∞: {predict['release_year']}")
        show_main_menu(message)

    elif message.text.startswith("–£–∑–Ω–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ üìù"):
        bot.send_message(message.chat.id, f"–û–ø–∏—Å–∞–Ω–∏–µ: {predict['synopsis']}")
        show_main_menu(message)

    elif message.text.startswith("–£–∑–Ω–∞—Ç—å –∂–∞–Ω—Ä üé¨"):
        bot.send_message(message.chat.id, f"–ñ–∞–Ω—Ä: {predict['genre']}")
        show_main_menu(message)

    elif message.text.startswith("–ü–æ–ª—É—á–∏—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ —Ñ–∏–ª—å–º üåê"):
        bot.send_message(message.chat.id,"https://www.imdb.com/title/tt0944947/plotsummary/")
        show_main_menu(message)

    else:
        bot.send_message(message.chat.id, text="–ù–∞ —Ç–∞–∫—É—é –∫–æ–º–∞–Ω–¥—É —è –Ω–µ –∑–∞–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω..")
        show_main_menu(message)



@bot.message_handler(func=lambda message: True)
def handle_other_messages(message):
    pass  


bot.polling(none_stop=True)